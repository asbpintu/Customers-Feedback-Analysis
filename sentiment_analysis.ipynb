{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting Path\n",
    "\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\asbpi\\Desktop\\Nit_DS & AI\\MY Projects\\project_sentiment analysis')\n",
    "\n",
    "# Import Packages\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "punct = string.punctuation\n",
    "stem = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Flatten, Dense, LSTM\n",
    "'''\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "\n",
    "root = pd.read_csv(r'amazon_alexa.tsv' , delimiter = '\\t' , quoting = 3)\n",
    "data = root.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe79786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ind and Dep variable\n",
    "\n",
    "\n",
    "data = data.drop(['rating','date','variation'],axis = 1)\n",
    "data.columns = ['reviews' , 'target']\n",
    "\n",
    "x = data['reviews']\n",
    "y = data['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_text\n",
    "\n",
    "\n",
    "def replace_text(rev):\n",
    "    \n",
    "    reviews = re.sub(r\"what's\", \"what is \", rev)\n",
    "    reviews = re.sub(r\"\\'s\", \" is\", reviews)\n",
    "    reviews = re.sub(r\"\\'ve\", \" have \", reviews)\n",
    "    reviews = re.sub(r\"can't\", \"cannot \", reviews)\n",
    "    reviews = re.sub(r\"n't\", \" not \", reviews)\n",
    "    reviews = re.sub(r\"i'm\", \"i am \", reviews)\n",
    "    reviews = re.sub(r\"\\'re\", \" are \", reviews)\n",
    "    reviews = re.sub(r\"\\'d\", \" would \", reviews)\n",
    "    reviews = re.sub(r\"\\'ll\", \" will \", reviews)\n",
    "    reviews = re.sub(r\"\\'scuse\", \" excuse \", reviews)\n",
    "    reviews = re.sub('\\W', ' ', reviews)\n",
    "    reviews = re.sub('\\s+', ' ', reviews)\n",
    "    reviews = reviews.strip(' ')\n",
    "    \n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79592646",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)) :\n",
    "    x[i] = replace_text(x[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a057b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_text\n",
    "\n",
    "\n",
    "def cleaned_text(rev):\n",
    "      \n",
    "    reviews = re.sub(r'\\[[0-9]*\\]', ' ',rev)\n",
    "    reviews = re.sub(r'\\s+', ' ', reviews)\n",
    "    reviews = re.sub('[^a-zA-Z]', ' ', reviews )\n",
    "    reviews = re.sub(r'\\s+', ' ', reviews)\n",
    "    reviews = re.sub(r'\\W*\\b\\w{1,3}\\b', \"\",reviews)\n",
    "    reviews = reviews.strip()\n",
    "    \n",
    "  \n",
    "    return reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)) :\n",
    "    x[i] = cleaned_text(x[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c612e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(rev):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = word_tokenize(rev)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    reviews = ' '.join(tokens)\n",
    "    \n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce217746",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)) :\n",
    "    x[i] = remove_stopwords(x[i])\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d528658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize  \n",
    "    \n",
    "def lemmatize(rev):\n",
    "    \n",
    "    doc = nlp(rev)\n",
    "    reviews = [words.lemma_ for words in doc]\n",
    "    reviews = ' '.join(reviews)\n",
    "    \n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b87c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(x)) :\n",
    "    x[i] = lemmatize(x[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c32fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = data.copy()\n",
    "\n",
    "# file_path = r'C:\\Users\\asbpi\\Desktop\\Nit_DS & AI\\MY Projects\\project_sentiment analysis\\new_data.csv'\n",
    "# new_data.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f156b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d783b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_reviews = data['reviews']\n",
    "pos_reviews = data['reviews'][data['target'] == 1]\n",
    "neg_reviews = data['reviews'][data['target'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b83543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_reviews = ' '.join(all_reviews .tolist())\n",
    "pos_reviews = ' '.join(pos_reviews .tolist())\n",
    "neg_reviews = ' '.join(neg_reviews .tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e18df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(data['target'].value_counts() , labels=['Positive','Negative'] , autopct='%1.0f%%')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_wordcloud = WordCloud(random_state=42 , max_font_size=100).generate(all_reviews)\n",
    "plt.imshow(all_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_wordcloud = WordCloud(random_state=42 , max_font_size=100).generate(pos_reviews)\n",
    "plt.imshow(pos_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neg_wordcloud = WordCloud(random_state=42 , max_font_size=100).generate(neg_reviews)\n",
    "plt.imshow(neg_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6674484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fre dist\n",
    "\n",
    "\n",
    "fredi=nltk.word_tokenize(all_reviews)\n",
    "freqDist = FreqDist(fredi)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xticks([])\n",
    "freqDist.plot(50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26329690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba62b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer().fit(x)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X = vectorizer.transform(x).toarray()\n",
    "\n",
    "X_c = pd.DataFrame(X , columns= feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d099b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer().fit(x)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X = vectorizer.transform(x).toarray()\n",
    "\n",
    "X_t = pd.DataFrame(X , columns= feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60809d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spliting\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y, test_size = 0.20, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61175464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logit.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79740883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30761c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Byas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Bernoulli Naive Bayes classifier\n",
    "berNB = BernoulliNB()\n",
    "berNB.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = berNB.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287863c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b637794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d706c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Gaussian Naive Bayes classifier\n",
    "gauNB = GaussianNB()\n",
    "gauNB.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = gauNB.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c457c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Multinomial Naive Bayes classifier\n",
    "mulNB = MultinomialNB()\n",
    "mulNB.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mulNB.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a519c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae67e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decession Tree\n",
    "\n",
    "# Train the DecisionTreeClassifier\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dectree.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0cb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4649135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "randForest = RandomForestClassifier()\n",
    "randForest.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = randForest.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efc28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
